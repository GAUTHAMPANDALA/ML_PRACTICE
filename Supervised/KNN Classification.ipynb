{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification using Scikit-learn\n",
    "\n",
    "LINK: https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "\n",
    "K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms.\n",
    "\n",
    "KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. \n",
    "\n",
    "In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. \n",
    "\n",
    "In political science, classifying potential voters in two classes will vote or wonâ€™t vote.\n",
    "\n",
    "KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "KNN is a non-parametric and lazy learning algorithm. \n",
    "\n",
    "Non-parametric means there is no assumption for underlying data distribution.In other words, the model structure determined from the dataset. \n",
    "\n",
    "This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. \n",
    "\n",
    "All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory.\n",
    "\n",
    "In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN algorithm working\n",
    "\n",
    "In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1.\n",
    "\n",
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/Knn_k1_z96jba.png\">\n",
    "\n",
    "Suppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n",
    "\n",
    "1. Calculate distance\n",
    "2. Find closest neighbors\n",
    "3. Vote for labels\n",
    "\n",
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\">\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we decide the number of neighbors in KNN?\n",
    "\n",
    "\n",
    "\n",
    "The number of neighbors(K) in KNN is a hyperparameter that you need choose at the time of model building. You can think of K as a controlling variable for the prediction model.\n",
    "\n",
    "Research has shown that no optimal number of neighbors suits all kind of data sets. Each dataset has it's own requirements. In the case of a small number of neighbors, the noise will have a higher influence on the result, and a large number of neighbors make it computationally expensive. Research has also shown that a small amount of neighbors are most flexible fit which will have low bias but high variance and a large number of neighbors will have a smoother decision boundary which means lower variance but higher bias.\n",
    "\n",
    "Generally, Data scientists choose as an odd number if the number of classes is even. You can also check by generating the model on different values of k and check their performance. You can also try Elbow method here.\n",
    "\n",
    "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP-1\n",
    "\n",
    "# Defining dataset\n",
    "# Assigning features and label variables\n",
    "# First Feature\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "# Second Feature\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "# Label or target varible\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#STEP-2\n",
    "\n",
    "#Encoding data columns\n",
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "weather_encoded=le.fit_transform(weather)\n",
    "print(weather_encoded)\n",
    "\n",
    "# converting string labels into numbers \n",
    "#-->you imported preprocessing module and created Label Encoder object.\n",
    "#Using this LabelEncoder object, you can fit and transform \"ALL\" column into the numeric column.\n",
    "\n",
    "temp_encoded=le.fit_transform(temp)\n",
    "label=le.fit_transform(play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP_3\n",
    "\n",
    "#Combining Features --> WE will combine multiple columns or features into a single set of data using \"zip\" function\n",
    "\n",
    "features=list(zip(weather_encoded,temp_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERE LABELS ARE \n",
    "\n",
    "    1.   a) MILD -->2    b)Hot -->0        c)Cold --> 1  (TEMP)\n",
    "    2.   a) Sunny -->2   b)Overcast -->0  c)Rainny --> 1  (WEATHER)\n",
    "    3.   a)Yes --> 1     b)No --> 0                       (OUTCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play\n"
     ]
    }
   ],
   "source": [
    "#STEP_4\n",
    "\n",
    "#Generating Model\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,0]]) # wethear and temp are parameters\n",
    "\n",
    "if(predicted==0):\n",
    "    print(\"NO PLAYING\")\n",
    "else:\n",
    "    print(\"Play\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with Multiple Labels\n",
    "\n",
    "\n",
    "we will learn about KNN with multiple classes.\n",
    "\n",
    "In the model the building part, you can use the wine dataset, which is a very famous multi-class classification problem. This data is the result of a chemical analysis of wines grown in the same region in Italy using three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "The dataset comprises 13 features ('alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline') and a target (type of cultivars).\n",
    "\n",
    "This data has three types of cultivar classes: 'class_0', 'class_1', and 'class_2'. Here, you can build a model to classify the type of cultivar. The dataset is available in the scikit-learn library, or you can also download it from the UCI Machine Learning Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "\n",
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the features\n",
    "print(wine.feature_names)\n",
    "print()\n",
    "# print the label species(class_0, class_1, class_2)\n",
    "print(wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wine data\n",
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n",
      "\n",
      "the wine labels\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the wine data (top 5 records)\n",
    "print (\"the wine data\")\n",
    "print(wine.data[0:5])\n",
    "print()\n",
    "# print the wine labels (0:Class_0, 1:Class_1, 2:Class_3)\n",
    "print (\"the wine labels\")\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n",
      "(178,)\n"
     ]
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "print(wine.data.shape)\n",
    "# print target(or label)shape\n",
    "print(wine.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted values are as \n",
      " [1 1 0 0 1 0 0 0 1 1 1 1 2 0 0 0 0 1 2 2 1 0 0 1 2 2 0 0 1 0 0 0 2 1 1 1 1\n",
      " 2 2 0 2 1 0 2 0 1 2 2 1 2 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Generating Model for K=5\n",
    "\n",
    "#Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier of n neighbours \n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"predicted values are as \\n\",y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation for k=5\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
